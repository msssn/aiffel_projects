{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "natural-amber",
   "metadata": {},
   "source": [
    "# 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-resort",
   "metadata": {},
   "source": [
    "## 1. 데이터 다운로드\n",
    "```~/aiffel/lyricist/data/lyrics```에 데이터가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-generation",
   "metadata": {},
   "source": [
    "## 2. 데이터 읽어오기\n",
    "* ```glob```를 활용해서 모든 txt파일을 읽어온 후,\n",
    "* ```raw_corpus```리스트에 문장 단위로 저장!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "established-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "inappropriate-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library import\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-steps",
   "metadata": {},
   "source": [
    "## 3. 데이터 전처리\n",
    "* ```preprocess_sentence()``` 함수를 활용해서 데이터를 정제!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-illinois",
   "metadata": {},
   "source": [
    "### 3-1. 데이터 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "respiratory-swiss",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['At first I was afraid', 'I was petrified', 'I kept thinking I could never live without you', 'By my side But then I spent so many nights', \"Just thinking how you've done me wrong\", 'I grew strong', \"I learned how to get along And so you're back\", 'From outer space', 'I just walked in to find you', 'Here without that look upon your face I should have changed that fucking lock']\n"
     ]
    }
   ],
   "source": [
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-saturn",
   "metadata": {},
   "source": [
    "* 특별히 처음부터 제거할만한 데이터(문장)는 없는 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-cuisine",
   "metadata": {},
   "source": [
    "### 3-2. Tokenise\n",
    "* 소문자로 바꾸고, 양쪽 공백을 지움 #1\n",
    "\n",
    "* 특수문자 양쪽에 공백넣기 #2\n",
    "\n",
    "* 여러개의 공백을 하나의 공백으로 바꿈 #3\n",
    "  \n",
    "* a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿈 #4\n",
    "  \n",
    "* 다시 양쪽 공백을 지움 #5\n",
    "\n",
    "* 문장 시작에는 ```<start>```, 끝에는 ```<end>```를 추가 #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "brown-termination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n",
      "<start> just thinking how you ve done me wrong <end>\n",
      "<start> here without that look upon your face i should have changed that fucking lock <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 한번 사용해보자.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))\n",
    "print(preprocess_sentence(\"Just thinking how you've done me wrong\")) # 라인9\n",
    "print(preprocess_sentence(\"Here without that look upon your face I should have changed that fucking lock\")) # 라인10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-graham",
   "metadata": {},
   "source": [
    "### 3-3. 단어사전 만들기 (데이터셋 구축)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "static-baking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> at first i was afraid <end>',\n",
       " '<start> i was petrified <end>',\n",
       " '<start> i grew strong <end>',\n",
       " '<start> from outer space <end>',\n",
       " '<start> i just walked in to find you <end>',\n",
       " '<start> walk out the door <end>',\n",
       " '<start> just turn around <end>',\n",
       " '<start> did you think i d crumble ? <end>',\n",
       " '<start> i will survive <end>',\n",
       " '<start> yeah <end>',\n",
       " '<start> i ve got all my life to live <end>',\n",
       " '<start> i ve got all my love to give <end>',\n",
       " '<start> yeah , yeah <end>',\n",
       " '<start> of my broken heart <end>',\n",
       " '<start> and i spent oh so many nights <end>',\n",
       " '<start> and you see me <end>',\n",
       " '<start> still in love with you <end>',\n",
       " '<start> for someone whose lovin me <end>',\n",
       " '<start> well now go , <end>',\n",
       " '<start> weren t you the one <end>',\n",
       " '<start> oh not i , <end>',\n",
       " '<start> i will survive yeah <end>',\n",
       " '<start> i know i ll be alive <end>',\n",
       " '<start> i will survive , <end>',\n",
       " '<start> i will survive <end>',\n",
       " '<start> yeah , yeah <end>',\n",
       " '<start> she s putting up her hair <end>',\n",
       " '<start> she s touring the facilities <end>',\n",
       " '<start> and picking up slack <end>',\n",
       " '<start> she s changing her name <end>',\n",
       " '<start> from kitty to karen <end>',\n",
       " '<start> he s going for speed <end>',\n",
       " '<start> she s all alone all alone <end>',\n",
       " '<start> he s going the distance <end>',\n",
       " '<start> he s going for speed <end>',\n",
       " '<start> she s all alone all alone <end>',\n",
       " '<start> he s going for speed <end>',\n",
       " '<start> up <end>',\n",
       " '<start> yo , shut the fuck up <end>',\n",
       " '<start> shut the fuck <end>',\n",
       " '<start> shut the fuck <end>',\n",
       " '<start> up <end>',\n",
       " '<start> yo , shut the fuck up <end>',\n",
       " '<start> shut the fuck <end>',\n",
       " '<start> shut the fuck <end>',\n",
       " '<start> yow , yow , yow <end>',\n",
       " '<start> all right , okay <end>',\n",
       " '<start> i don t wanna <end>',\n",
       " '<start> shut the fuck <end>',\n",
       " '<start> shut the fuck <end>',\n",
       " '<start> i don t wanna <end>',\n",
       " '<start> shut the fuck <end>',\n",
       " '<start> yeah , ho , yow <end>',\n",
       " '<start> shut the fuck <end>',\n",
       " '<start> i was petrified <end>',\n",
       " '<start> i grew strong <end>',\n",
       " '<start> from outer space <end>',\n",
       " '<start> i just walked in to find you <end>',\n",
       " '<start> walk out the door <end>',\n",
       " '<start> just turn around <end>',\n",
       " '<start> did you think i d crumble ? <end>',\n",
       " '<start> i will survive <end>',\n",
       " '<start> yeah <end>',\n",
       " '<start> i ve got all my life to live <end>',\n",
       " '<start> i ve got all my love to give <end>',\n",
       " '<start> yeah , yeah <end>',\n",
       " '<start> of my broken heart <end>',\n",
       " '<start> and i spent oh so many nights <end>',\n",
       " '<start> and you see me <end>',\n",
       " '<start> still in love with you <end>',\n",
       " '<start> for someone whose lovin me <end>',\n",
       " '<start> well now go , <end>',\n",
       " '<start> weren t you the one <end>',\n",
       " '<start> oh not i , <end>',\n",
       " '<start> i will survive yeah <end>',\n",
       " '<start> i know i ll be alive <end>',\n",
       " '<start> i will survive , <end>',\n",
       " '<start> i will survive <end>',\n",
       " '<start> yeah , yeah <end>',\n",
       " '<start> she s putting up her hair <end>',\n",
       " '<start> she s touring the facilities <end>',\n",
       " '<start> and picking up slack <end>',\n",
       " '<start> she s changing her name <end>',\n",
       " '<start> from kitty to karen <end>',\n",
       " '<start> he s going for speed <end>',\n",
       " '<start> she s all alone all alone <end>',\n",
       " '<start> he s going the distance <end>',\n",
       " '<start> he s going for speed <end>',\n",
       " '<start> she s all alone all alone <end>',\n",
       " '<start> he s going for speed <end>',\n",
       " '<start> up <end>',\n",
       " '<start> yo , shut the fuck up <end>',\n",
       " '<start> shut the fuck <end>',\n",
       " '<start> shut the fuck <end>',\n",
       " '<start> up <end>',\n",
       " '<start> yo , shut the fuck up <end>',\n",
       " '<start> shut the fuck <end>',\n",
       " '<start> shut the fuck <end>',\n",
       " '<start> yow , yow , yow <end>',\n",
       " '<start> all right , okay <end>']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    # 지나치게 긴 문장은 과도한 padding을 갖게 하므로 제거합니다.\n",
    "    # 지나치게 긴 문장: 토큰의 개수가 15개를 넘어가는 문장\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    if len(sentence) >= 30: continue\n",
    "        \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 100개만 확인\n",
    "corpus[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-working",
   "metadata": {},
   "source": [
    "### 3-4. 데이터를 숫자로 변환\n",
    "* 텐서플로우 모듈을 활용한 vectorise\n",
    "* 단어장의 크기는 12000이상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "corresponding-pressure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   76  321 ...    0    0    0]\n",
      " [   2    4   60 ...    0    0    0]\n",
      " [   2    4 1831 ...    0    0    0]\n",
      " ...\n",
      " [   2 2050    8 ...    0    0    0]\n",
      " [   2    4   21 ...    0    0    0]\n",
      " [   2   50   16 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f77b33d17d0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 1. 단어장 만들기\n",
    "    # 1-1. Tokenise\n",
    "    # 12000단어를 기억할 수 있는 tokenizer를 만듦\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없음\n",
    "    # 12000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # 1-2. corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # 2. Translation (corpus -> Tensor)\n",
    "    # 2-1. 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    # 2-2. 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # **문장 뒤에 패딩을 붙여 길이를 맞춰줍니다. (만약 시퀀스가 짧다면)\n",
    "    # **문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "detected-damage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  76 321   4  60 636   3   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0]\n",
      "[ 76 321   4  60 636   3   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0]\n"
     ]
    }
   ],
   "source": [
    "# 텐서를 소스와 타겟으로 분리\n",
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "src_input = tensor[:, :-1]  \n",
    "\n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0]) # 소스 문장\n",
    "print(tgt_input[0]) # 타겟 문장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-peoples",
   "metadata": {},
   "source": [
    "### 3-5. 데이터셋 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "prescribed-jamaica",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 19), (256, 19)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "# tokenizer가 구축한 단어사전 내 12000개 + 0:<pad> = 12001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset # 데이터셋 준비 완료! 전처리 끝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-teacher",
   "metadata": {},
   "source": [
    "### Step 3-6. 데이터셋 분리\n",
    "* 총 데이터의 20%를 평가데이터셋으로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dimensional-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, shuffle=True, stratify=None, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "insured-smoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (54672, 19)\n",
      "Target Train: (54672, 19)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-mechanics",
   "metadata": {},
   "source": [
    "## 4. 인공지능 만들기\n",
    "* Embedding Size와 Hidden Size를 조절하며, 10 Epoch안에 ```val_loss``` 값을 2.2 수준으로 줄일 수 있는 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "biological-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 512\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "incredible-public",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "266/266 [==============================] - 322s 1s/step - loss: 2.3096\n",
      "Epoch 2/10\n",
      "266/266 [==============================] - 305s 1s/step - loss: 1.5219\n",
      "Epoch 3/10\n",
      "266/266 [==============================] - 307s 1s/step - loss: 1.3990\n",
      "Epoch 4/10\n",
      "266/266 [==============================] - 308s 1s/step - loss: 1.2976\n",
      "Epoch 5/10\n",
      "266/266 [==============================] - 308s 1s/step - loss: 1.2057\n",
      "Epoch 6/10\n",
      "266/266 [==============================] - 308s 1s/step - loss: 1.1248\n",
      "Epoch 7/10\n",
      "266/266 [==============================] - 308s 1s/step - loss: 1.0491\n",
      "Epoch 8/10\n",
      "266/266 [==============================] - 307s 1s/step - loss: 0.9848\n",
      "Epoch 9/10\n",
      "266/266 [==============================] - 307s 1s/step - loss: 0.9310\n",
      "Epoch 10/10\n",
      "266/266 [==============================] - 307s 1s/step - loss: 0.8794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7719a65c10>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-astrology",
   "metadata": {},
   "source": [
    "## 5. 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "blessed-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "younger-ferry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-telling",
   "metadata": {},
   "source": [
    "## 6. Comments\n",
    "* 처음에 에폭수를 100으로 해서 시도했을 때는 loss는 지금보다 높고, i love the way you lie를 출력해 주었다.\n",
    "* 지금은 loss가 확 낮아지고 I love you가 뜬 것으로 보아, I love 다음에 you가 나오는 경우가 상대적으로 더 많은 것 같다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
